---
title: "Running Inference with FastVLM"
description: "Learn how to use a pretrained FastVLM checkpoint for vision-language inference, walking through the process of setting up your environment, selecting a model, and generating outputs from image and text prompts using the command-line interface."
---

# Running Inference with FastVLM

Unlock the power of FastVLM by running inference on your images with pretrained checkpoints. This guide walks you through setting up your environment, selecting a model, and generating accurate, efficient vision-language outputs using the command-line interface.

---

## Workflow Overview

### What You'll Achieve
By the end of this guide, you'll be able to run FastVLM inference on local images, combine image and text prompts seamlessly, and view rich natural language descriptions or answers generated by the model.

### Prerequisites
- A Python 3.10 environment (conda recommended)
- FastVLM installed in an active environment
- Access to pretrained FastVLM checkpoint files downloaded and organized
- Basic familiarity with command-line interface usage

### Expected Outcome
- Successfully run inference commands that ingest an image and prompt text
- Generate and display model output describing the image or answering queries
- Understand adjustable inference parameters like temperature and beam search

### Time to Complete
About 10–15 minutes, assuming prerequisites are ready.

### Difficulty Level
Beginner to Intermediate - no programming required, just command-line interaction.

---

## Step-by-Step Instructions

### 1. Prepare Your Environment

Activate your FastVLM Python environment. If you have not created one, use:
```bash
conda create -n fastvlm python=3.10
conda activate fastvlm
pip install -e .
```

### 2. Download Pretrained Model Checkpoints

Ensure model checkpoints are downloaded to a directory, for example, `checkpoints/`:
```bash
bash get_models.sh
```
This downloads all supported pretrained FastVLM models.

### 3. Select the Model and Image

Choose the FastVLM model variant and stage checkpoint you want to use, for example:
- `fastvlm_0.5b_stage3`
- `fastvlm_1.5b_stage3`
- `fastvlm_7b_stage3`

Prepare the image you want to infer on, such as `example.png`.

### 4. Run Inference Using `predict.py`

The simplest inference command looks like this:
```bash
python predict.py --model-path checkpoints/fastvlm_0.5b_stage3 \
                  --image-file /path/to/example.png \
                  --prompt "Describe the image."
```
Replace `/path/to/example.png` with your image file path.

#### Key Parameters:
- `--model-path`: Directory containing the pretrained checkpoint
- `--image-file`: Path or URL of the image
- `--prompt`: Text prompt to guide the model output
- Optional parameters:
  - `--temperature`: Controls randomness (default 0.2)
  - `--top_p`: Nucleus sampling probability
  - `--num_beams`: Beam search width for better quality output
  - `--conv-mode`: Conversation mode preset (default is `qwen_2`)

### 5. Understand the Output

After the command runs, the model generates a text output describing the image or answering your prompt. This appears directly in your terminal.

### 6. Adjust Inference Settings for Your Needs

For more creative or diverse outputs, increase `--temperature` and decrease `--num_beams`. For more precise, deterministic results, reduce temperature to 0 and increase beams.

---

## Practical Examples

### Basic Description
```bash
python predict.py --model-path checkpoints/fastvlm_0.5b_stage3 \
                  --image-file test_images/cat.png \
                  --prompt "What is in this image?"
```
Output:
> A cute tabby cat sitting on a windowsill.

### Counting Objects
```bash
python predict.py --model-path checkpoints/fastvlm_1.5b_stage3 \
                  --image-file test_images/crowd.jpg \
                  --prompt "How many people are in this image?"
```
Output:
> There are approximately 12 people in this image.

### Using Beam Search
```bash
python predict.py --model-path checkpoints/fastvlm_7b_stage3 \
                  --image-file test_images/sunset.png \
                  --prompt "Describe the scene." \
                  --num_beams 5
```
Output:
> A vibrant sunset over the ocean with orange and purple hues.

---

## Tips & Best Practices

- Keep images in RGB format for best results.
- Use high-resolution images but be mindful of memory limits.
- When uncertain about parameter values, start with defaults and incrementally tweak.
- Ensure `--model-path` points to a fully extracted checkpoint directory.
- Use smaller models for faster inference and experimentation.

---

## Troubleshooting

<AccordionGroup title="Common Inference Issues and Solutions">
<Accordion title="Model Path Not Found">
Check that the path passed to `--model-path` exists and contains checkpoint files (*.ckpt, *.pt, or folders with model files).

Use absolute or relative paths carefully.
</Accordion>
<Accordion title="Image File Errors">
Ensure the image file exists and is accessible. Supported formats include PNG, JPG.

For remote images, use local downloads for reliability.
</Accordion>
<Accordion title="GPU / Device Issues">
Default device is Apple Silicon (`mps`). Adjust device selection if needed in code or environment.

On non-Apple devices, ensure PyTorch supports your GPU.
</Accordion>
<Accordion title="Tokenization or Prompt Issues">
Use simple prompts initially. Avoid unsupported special characters.

If generation stalls or output is empty, try lowering temperature or increasing `max_new_tokens`.
</Accordion>
<Accordion title="Performance and Latency">
For faster startup, use smaller checkpoints.

For repeated runs, keep environment activated and models cached.
</Accordion>
</AccordionGroup>

---

## Next Steps & Related Documentation

- [Downloading Pretrained Models](../getting-started/setup-local-inference/download-models) — Get pretrained checkpoint files.
- [Installation & Environment Setup](../getting-started/setup-local-inference/installation) — Prepare the Python environment.
- [Exporting Models for Apple Silicon](../getting-started/apple-device-inference/apple-silicon-export) — Convert models for optimized Apple hardware inference.
- [Running the Demo App on iPhone, iPad, or Mac](../getting-started/apple-device-inference/running-mobile-app) — Deploy FastVLM on Apple devices.
- [Optimizing Latency and Throughput](../../guides/performance-best-practices/latency-throughput-tips) — Tune inference for best performance.
- [Interactive Chat with FastVLM via Web Interface](../../guides/core-workflows/using-gradio-web) — For conversational visual language applications.

---

## Additional Resources

- Official Paper: [FastVLM CVPR 2025](https://www.arxiv.org/abs/2412.13303)
- Source code: [GitHub Repository](https://github.com/apple/ml-fastvlm)
- Model Zoo and Checkpoints: Provided in the base repository

---

Congratulations! You now have everything you need to run FastVLM inference and generate insightful descriptions from images with ease.